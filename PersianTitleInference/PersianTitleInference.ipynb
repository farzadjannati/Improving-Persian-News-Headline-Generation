{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOgeukV5Fa8N"
   },
   "source": [
    "<div style=\"display:block;width:100%;margin:auto;\" direction=rtl align=center>\n",
    "    <br><br>\n",
    "    <div style=\"width:100%;margin:100;display:block;background-color:#fff0;\" display=block align=center>\n",
    "        <table style=\"border-style:hidden;border-collapse:collapse;\">\n",
    "            <tr>\n",
    "                <td style=\"border: none!important;\">\n",
    "                    <img width=130 align=right src=\"https://i.ibb.co/yXKQmtZ/logo1.png\" style=\"margin:0;\" />\n",
    "                </td>\n",
    "                <td style=\"text-align:center;border: none!important;\">\n",
    "                    <h1 align=center><font size=5 color=\"#045F5F\"> <b> Large Language Models (LLM)</b><br><br>Final Project</font></h1>\n",
    "                </td>\n",
    "                <td style=\"border: none!important;\">\n",
    "                    <img width=170 align=left src=\"https://i.ibb.co/wLjqFkw/logo2.png\" style=\"margin:0;\" />\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        <h1> Farzad Jannati- Abolfazl Asarian Nejad-Shahriar Rahimi Rad </h1>\n",
    "        <h1> Prof. MJ. Dousti & Yadollah Yaghoobzadeh </h1>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruIIPk6sB0Kq"
   },
   "source": [
    "# Persian News Title Generation with Fine-Tuned Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "iRDqvDbACHOq"
   },
   "source": [
    ">[Persian News Title Generation with Fine-Tuned Llama 3](#scrollTo=ruIIPk6sB0Kq)\n",
    "\n",
    ">>[Title Generation Inference](#scrollTo=iuDItTAmBio6)\n",
    "\n",
    ">>>[Import required libraries](#scrollTo=HysyMwHNBqpW)\n",
    "\n",
    ">>>[Hugging Face Authentication](#scrollTo=5Z-hzQghV0RT)\n",
    "\n",
    ">>>[Initialization and Pipeline Configuration](#scrollTo=_qFVhFwPBwzq)\n",
    "\n",
    ">>>[Loading and Preprocessing the Hamshahri Dataset](#scrollTo=uOGn5ueZCaOi)\n",
    "\n",
    ">>>[Generating a Title for a Single Text Input](#scrollTo=kH5E1BL8Cot2)\n",
    "\n",
    ">>>[Performing Batch Inference on the Dataset](#scrollTo=tzdYUg8nCqag)\n",
    "\n",
    ">>>[Saving Results to Multiple Formats (Excel, CSV, JSON, TXT)](#scrollTo=tM4VxYEICxRN)\n",
    "\n",
    ">>>[Basic Analysis of the Generated Results](#scrollTo=ALqy38WzC6KI)\n",
    "\n",
    ">>>[Main Function: Executing the Full Inference Pipeline](#scrollTo=BYThP65ADJFn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuDItTAmBio6"
   },
   "source": [
    "## Title Generation Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HysyMwHNBqpW"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xrV4p2t6U5Qd"
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1g4SNMsJV_0w"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0LnPTRRKBb_P"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z-hzQghV0RT"
   },
   "source": [
    "### Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "i-lbYf8RVxTV"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "def authenticate_huggingface():\n",
    "    \"\"\"\n",
    "    Authenticate with Hugging Face using access token\n",
    "    \"\"\"\n",
    "    # Enter your Hugging Face token here\n",
    "    HF_TOKEN = input(\"Please enter your Hugging Face access token: \")\n",
    "\n",
    "    try:\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"✓ Successfully authenticated with Hugging Face!\")\n",
    "        return HF_TOKEN  # Return the token to use later\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Authentication failed: {e}\")\n",
    "        print(\"Please make sure your token is correct and has access to Llama models\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qFVhFwPBwzq"
   },
   "source": [
    "### Initialization and Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "sdNOgVyrCY19"
   },
   "outputs": [],
   "source": [
    "class PersianTitleInference:\n",
    "    \"\"\"\n",
    "    A class for performing inference on Persian news articles using a fine-tuned Llama 3 model\n",
    "    with LoRA adapters to generate appropriate titles.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adapter_path=\".\", base_model_name=None, device=None, hf_token=None):\n",
    "        \"\"\"\n",
    "        Initialize the inference pipeline with LoRA adapters.\n",
    "\n",
    "        Args:\n",
    "            adapter_path (str): Path to the directory containing LoRA adapter files\n",
    "            base_model_name (str): Name of the base model (if None, will be loaded from adapter config)\n",
    "            device (str): Device to run inference on ('cuda' or 'cpu')\n",
    "            hf_token (str): Hugging Face authentication token\n",
    "        \"\"\"\n",
    "        self.adapter_path = adapter_path\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_model_name = base_model_name\n",
    "        self.hf_token = hf_token\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.model_loaded = False\n",
    "\n",
    "        # Inference template for Llama 3\n",
    "        self.inference_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "        self.system_message = \"you are an ai assistant to generate persian title for given article\"\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Load the base model and apply LoRA adapters for inference.\n",
    "        \"\"\"\n",
    "        if self.model_loaded:\n",
    "            print(\"Model already loaded.\")\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            print(f\"Loading PEFT configuration from {self.adapter_path}...\")\n",
    "\n",
    "            # Load the PEFT configuration to get base model name if not provided\n",
    "            peft_config = PeftConfig.from_pretrained(self.adapter_path)\n",
    "            if not self.base_model_name:\n",
    "                self.base_model_name = peft_config.base_model_name_or_path\n",
    "            print(f\"Using base model: {self.base_model_name}\")\n",
    "\n",
    "            # Configure 4-bit quantization for memory efficiency\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "\n",
    "            print(f\"Loading base model: {self.base_model_name}\")\n",
    "            # Load base model with quantization and token\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                token=self.hf_token  # Use the stored token\n",
    "            )\n",
    "\n",
    "            # Load tokenizer\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                token=self.hf_token  # Use the stored token\n",
    "            )\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "            # Apply LoRA adapters\n",
    "            print(f\"Applying LoRA adapters from {self.adapter_path}...\")\n",
    "            self.model = PeftModel.from_pretrained(self.model, self.adapter_path)\n",
    "\n",
    "            # Set model to evaluation mode\n",
    "            self.model.eval()\n",
    "\n",
    "            self.model_loaded = True\n",
    "            print(\"Model with LoRA adapters loaded successfully!\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_hamshahri_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess the Hamshahri news dataset.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the Excel file containing news data\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Preprocessed dataframe with news articles\n",
    "        \"\"\"\n",
    "        print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "        try:\n",
    "            # Load Excel file\n",
    "            df = pd.read_excel(file_path)\n",
    "\n",
    "            # Display dataset information\n",
    "            print(f\"Dataset shape: {df.shape}\")\n",
    "            print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "            # Clean and preprocess text\n",
    "            df['text'] = df['text'].fillna('')\n",
    "            df['introtext'] = df['introtext'].fillna('')\n",
    "            df['title'] = df['title'].fillna('')\n",
    "\n",
    "            # Create summary from introtext if available, otherwise use first part of text\n",
    "            df['summary'] = df.apply(\n",
    "                lambda row: row['introtext'] if row['introtext'] and len(row['introtext']) > 20\n",
    "                else row['text'][:500] if row['text'] else \"متن خبر موجود نیست\",\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            print(f\"Loaded {len(df)} news articles.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_title(self, text, max_new_tokens=64, temperature=0.7, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate a title for the given text using the model with LoRA adapters.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text (article or summary)\n",
    "            max_new_tokens (int): Maximum number of tokens to generate\n",
    "            temperature (float): Sampling temperature\n",
    "            top_p (float): Nucleus sampling parameter\n",
    "\n",
    "        Returns:\n",
    "            str: Generated title\n",
    "        \"\"\"\n",
    "        if not self.model_loaded:\n",
    "            print(\"Model not loaded. Loading model...\")\n",
    "            if not self.load_model():\n",
    "                return \"خطا در بارگذاری مدل\"\n",
    "\n",
    "        try:\n",
    "            # Prepare the prompt\n",
    "            user_message = f\"generate a proper title for this article:\\n{text}\"\n",
    "            prompt = self.inference_template.format(\n",
    "                system_message=self.system_message,\n",
    "                user_message=user_message\n",
    "            )\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate title\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            # Decode output\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Extract the generated title\n",
    "            assistant_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "            if assistant_tag in generated_text:\n",
    "                title = generated_text.split(assistant_tag)[-1].strip()\n",
    "            else:\n",
    "                # Try alternative extraction method\n",
    "                parts = generated_text.split(\"assistant\")\n",
    "                if len(parts) > 1:\n",
    "                    title = parts[-1].strip()\n",
    "                else:\n",
    "                    title = generated_text\n",
    "\n",
    "            # Clean up the title (remove any remaining tags or special tokens)\n",
    "            title = title.replace(\"<|eot_id|>\", \"\").strip()\n",
    "            title = title.split(\"\\n\")[0]  # Take only the first line\n",
    "\n",
    "            return title\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating title: {e}\")\n",
    "            return \"خطا در تولید عنوان\"\n",
    "\n",
    "    def batch_inference(self, df, input_column='summary', batch_size=4):\n",
    "        \"\"\"\n",
    "        Perform batch inference on the entire dataset.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing the articles\n",
    "            input_column (str): Column to use as input ('text' or 'summary')\n",
    "            batch_size (int): Number of samples to process in each batch\n",
    "\n",
    "        Returns:\n",
    "            list: List of generated titles\n",
    "        \"\"\"\n",
    "        print(f\"Starting batch inference on {len(df)} articles...\")\n",
    "        print(f\"Using '{input_column}' as input\")\n",
    "\n",
    "        generated_titles = []\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(df), batch_size), desc=\"Generating titles\"):\n",
    "            batch = df.iloc[i:i+batch_size]\n",
    "\n",
    "            for _, row in batch.iterrows():\n",
    "                input_text = row[input_column]\n",
    "\n",
    "                # Skip empty texts\n",
    "                if not input_text or len(str(input_text).strip()) < 10:\n",
    "                    generated_titles.append(\"متن ورودی ناکافی\")\n",
    "                    continue\n",
    "\n",
    "                # Truncate very long texts to prevent token overflow\n",
    "                if len(str(input_text)) > 2000:\n",
    "                    input_text = str(input_text)[:2000]\n",
    "\n",
    "                # Generate title\n",
    "                title = self.generate_title(input_text)\n",
    "                generated_titles.append(title)\n",
    "\n",
    "        return generated_titles\n",
    "\n",
    "    def save_results(self, df, generated_titles, output_path):\n",
    "        \"\"\"\n",
    "        Save the results to multiple formats for analysis.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Original dataframe\n",
    "            generated_titles (list): List of generated titles\n",
    "            output_path (str): Base path for saving results\n",
    "        \"\"\"\n",
    "        print(\"Saving results...\")\n",
    "\n",
    "        # Add generated titles to dataframe\n",
    "        df['generated_title'] = generated_titles\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Generate timestamp for filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_path = os.path.join(output_path, f'hamshahri_with_titles_{timestamp}.xlsx')\n",
    "        df.to_excel(excel_path, index=False)\n",
    "        print(f\"Saved Excel file: {excel_path}\")\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_path = os.path.join(output_path, f'hamshahri_with_titles_{timestamp}.csv')\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved CSV file: {csv_path}\")\n",
    "\n",
    "        # Save only the generated titles\n",
    "        titles_path = os.path.join(output_path, f'generated_titles_{timestamp}.txt')\n",
    "        with open(titles_path, 'w', encoding='utf-8') as f:\n",
    "            for title in generated_titles:\n",
    "                f.write(title + '\\n')\n",
    "        print(f\"Saved titles file: {titles_path}\")\n",
    "\n",
    "        # Save comparison JSON for analysis\n",
    "        comparison_data = []\n",
    "        for i, row in df.iterrows():\n",
    "            comparison_data.append({\n",
    "                'id': i,\n",
    "                'news_id': row.get('news_id', i),\n",
    "                'original_title': row['title'],\n",
    "                'generated_title': row['generated_title'],\n",
    "                'category': row.get('category', 'Unknown'),\n",
    "                'summary': row['summary'][:200] + '...' if len(row['summary']) > 200 else row['summary']\n",
    "            })\n",
    "\n",
    "        json_path = os.path.join(output_path, f'title_comparison_{timestamp}.json')\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comparison_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved comparison JSON: {json_path}\")\n",
    "\n",
    "    def analyze_results(self, df):\n",
    "        \"\"\"\n",
    "        Perform basic analysis on the generated titles.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with original and generated titles\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Results Analysis ===\")\n",
    "\n",
    "        # Basic statistics\n",
    "        print(f\"Total articles processed: {len(df)}\")\n",
    "\n",
    "        # Average title length\n",
    "        df['original_title_length'] = df['title'].astype(str).str.len()\n",
    "        df['generated_title_length'] = df['generated_title'].astype(str).str.len()\n",
    "\n",
    "        print(f\"Average original title length: {df['original_title_length'].mean():.1f} characters\")\n",
    "        print(f\"Average generated title length: {df['generated_title_length'].mean():.1f} characters\")\n",
    "\n",
    "        # Sample comparisons\n",
    "        print(\"\\n=== Sample Title Comparisons ===\")\n",
    "        samples = df.sample(min(5, len(df)))\n",
    "\n",
    "        for _, row in samples.iterrows():\n",
    "            print(f\"\\nNews ID: {row.get('news_id', 'N/A')}\")\n",
    "            print(f\"Category: {row.get('category', 'Unknown')}\")\n",
    "            print(f\"Original Title: {row['title']}\")\n",
    "            print(f\"Generated Title: {row['generated_title']}\")\n",
    "            print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOGn5ueZCaOi"
   },
   "source": [
    "### Loading and Preprocessing the Hamshahri Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ZbfN-RPICf1z"
   },
   "outputs": [],
   "source": [
    "def load_hamshahri_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess the Hamshahri news dataset.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the Excel file containing news data\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Preprocessed dataframe with news articles\n",
    "        \"\"\"\n",
    "        print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "        try:\n",
    "            # Load Excel file\n",
    "            df = pd.read_excel(file_path)\n",
    "\n",
    "            # Display dataset information\n",
    "            print(f\"Dataset shape: {df.shape}\")\n",
    "            print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "            # Clean and preprocess text\n",
    "            df['text'] = df['text'].fillna('')\n",
    "            df['introtext'] = df['introtext'].fillna('')\n",
    "            df['title'] = df['title'].fillna('')\n",
    "\n",
    "            # Create summary from introtext if available, otherwise use first part of text\n",
    "            df['summary'] = df.apply(\n",
    "                lambda row: row['introtext'] if row['introtext'] and len(row['introtext']) > 20\n",
    "                else row['text'][:500] if row['text'] else \"متن خبر موجود نیست\",\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            print(f\"Loaded {len(df)} news articles.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH5E1BL8Cot2"
   },
   "source": [
    "### Generating a Title for a Single Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "I-a9RwBICpMY"
   },
   "outputs": [],
   "source": [
    "def generate_title(self, text, max_new_tokens=64, temperature=0.7, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate a title for the given text using the model with LoRA adapters.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text (article or summary)\n",
    "            max_new_tokens (int): Maximum number of tokens to generate\n",
    "            temperature (float): Sampling temperature\n",
    "            top_p (float): Nucleus sampling parameter\n",
    "\n",
    "        Returns:\n",
    "            str: Generated title\n",
    "        \"\"\"\n",
    "        if not self.model_loaded:\n",
    "            print(\"Model not loaded. Loading model...\")\n",
    "            if not self.load_model():\n",
    "                return \"خطا در بارگذاری مدل\"\n",
    "\n",
    "        try:\n",
    "            # Prepare the prompt\n",
    "            user_message = f\"generate a proper title for this article:\\n{text}\"\n",
    "            prompt = self.inference_template.format(\n",
    "                system_message=self.system_message,\n",
    "                user_message=user_message\n",
    "            )\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate title\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            # Decode output\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Extract the generated title\n",
    "            assistant_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "            if assistant_tag in generated_text:\n",
    "                title = generated_text.split(assistant_tag)[-1].strip()\n",
    "            else:\n",
    "                # Try alternative extraction method\n",
    "                parts = generated_text.split(\"assistant\")\n",
    "                if len(parts) > 1:\n",
    "                    title = parts[-1].strip()\n",
    "                else:\n",
    "                    title = generated_text\n",
    "\n",
    "            # Clean up the title (remove any remaining tags or special tokens)\n",
    "            title = title.replace(\"<|eot_id|>\", \"\").strip()\n",
    "            title = title.split(\"\\n\")[0]  # Take only the first line\n",
    "\n",
    "            return title\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating title: {e}\")\n",
    "            return \"خطا در تولید عنوان\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzdYUg8nCqag"
   },
   "source": [
    "### Performing Batch Inference on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qJ3ypNCyCq09"
   },
   "outputs": [],
   "source": [
    "def batch_inference(self, df, input_column='summary', batch_size=4):\n",
    "        \"\"\"\n",
    "        Perform batch inference on the entire dataset.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing the articles\n",
    "            input_column (str): Column to use as input ('text' or 'summary')\n",
    "            batch_size (int): Number of samples to process in each batch\n",
    "\n",
    "        Returns:\n",
    "            list: List of generated titles\n",
    "        \"\"\"\n",
    "        print(f\"Starting batch inference on {len(df)} articles...\")\n",
    "        print(f\"Using '{input_column}' as input\")\n",
    "\n",
    "        generated_titles = []\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(df), batch_size), desc=\"Generating titles\"):\n",
    "            batch = df.iloc[i:i+batch_size]\n",
    "\n",
    "            for _, row in batch.iterrows():\n",
    "                input_text = row[input_column]\n",
    "\n",
    "                # Skip empty texts\n",
    "                if not input_text or len(str(input_text).strip()) < 10:\n",
    "                    generated_titles.append(\"متن ورودی ناکافی\")\n",
    "                    continue\n",
    "\n",
    "                # Truncate very long texts to prevent token overflow\n",
    "                if len(str(input_text)) > 2000:\n",
    "                    input_text = str(input_text)[:2000]\n",
    "\n",
    "                # Generate title\n",
    "                title = self.generate_title(input_text)\n",
    "                generated_titles.append(title)\n",
    "\n",
    "        return generated_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM4VxYEICxRN"
   },
   "source": [
    "### Saving Results to Multiple Formats (Excel, CSV, JSON, TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "JR0K6-0yC5IO"
   },
   "outputs": [],
   "source": [
    "def save_results(self, df, generated_titles, output_path):\n",
    "        \"\"\"\n",
    "        Save the results to multiple formats for analysis.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Original dataframe\n",
    "            generated_titles (list): List of generated titles\n",
    "            output_path (str): Base path for saving results\n",
    "        \"\"\"\n",
    "        print(\"Saving results...\")\n",
    "\n",
    "        # Add generated titles to dataframe\n",
    "        df['generated_title'] = generated_titles\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Generate timestamp for filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Save to Excel\n",
    "        excel_path = os.path.join(output_path, f'hamshahri_with_titles_{timestamp}.xlsx')\n",
    "        df.to_excel(excel_path, index=False)\n",
    "        print(f\"Saved Excel file: {excel_path}\")\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_path = os.path.join(output_path, f'hamshahri_with_titles_{timestamp}.csv')\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved CSV file: {csv_path}\")\n",
    "\n",
    "        # Save only the generated titles\n",
    "        titles_path = os.path.join(output_path, f'generated_titles_{timestamp}.txt')\n",
    "        with open(titles_path, 'w', encoding='utf-8') as f:\n",
    "            for title in generated_titles:\n",
    "                f.write(title + '\\n')\n",
    "        print(f\"Saved titles file: {titles_path}\")\n",
    "\n",
    "        # Save comparison JSON for analysis\n",
    "        comparison_data = []\n",
    "        for i, row in df.iterrows():\n",
    "            comparison_data.append({\n",
    "                'id': i,\n",
    "                'news_id': row.get('news_id', i),\n",
    "                'original_title': row['title'],\n",
    "                'generated_title': row['generated_title'],\n",
    "                'category': row.get('category', 'Unknown'),\n",
    "                'summary': row['summary'][:200] + '...' if len(row['summary']) > 200 else row['summary']\n",
    "            })\n",
    "\n",
    "        json_path = os.path.join(output_path, f'title_comparison_{timestamp}.json')\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comparison_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved comparison JSON: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALqy38WzC6KI"
   },
   "source": [
    "### Basic Analysis of the Generated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "owNQb8NhC6jz"
   },
   "outputs": [],
   "source": [
    "def analyze_results(self, df):\n",
    "        \"\"\"\n",
    "        Perform basic analysis on the generated titles.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with original and generated titles\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Results Analysis ===\")\n",
    "\n",
    "        # Basic statistics\n",
    "        print(f\"Total articles processed: {len(df)}\")\n",
    "\n",
    "        # Average title length\n",
    "        df['original_title_length'] = df['title'].astype(str).str.len()\n",
    "        df['generated_title_length'] = df['generated_title'].astype(str).str.len()\n",
    "\n",
    "        print(f\"Average original title length: {df['original_title_length'].mean():.1f} characters\")\n",
    "        print(f\"Average generated title length: {df['generated_title_length'].mean():.1f} characters\")\n",
    "\n",
    "        # Sample comparisons\n",
    "        print(\"\\n=== Sample Title Comparisons ===\")\n",
    "        samples = df.sample(min(5, len(df)))\n",
    "\n",
    "        for _, row in samples.iterrows():\n",
    "            print(f\"\\nNews ID: {row.get('news_id', 'N/A')}\")\n",
    "            print(f\"Category: {row.get('category', 'Unknown')}\")\n",
    "            print(f\"Original Title: {row['title']}\")\n",
    "            print(f\"Generated Title: {row['generated_title']}\")\n",
    "            print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WGzZ8SsgUXcl"
   },
   "outputs": [],
   "source": [
    "def verify_files():\n",
    "    \"\"\"\n",
    "    Verify that required files exist in the current directory\n",
    "    \"\"\"\n",
    "    required_files = ['adapter_config.json', 'adapter_model.safetensors', 'hamshahri_online_10.xlsx']\n",
    "\n",
    "    print(\"Checking for required files...\")\n",
    "    for file in required_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"✓ Found: {file}\")\n",
    "        else:\n",
    "            print(f\"✗ Missing: {file}\")\n",
    "            return False\n",
    "\n",
    "    print(\"All required files found!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYThP65ADJFn"
   },
   "source": [
    "### Main Function: Executing the Full Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2e5467e6b225446e8b10a60708c31d1f",
      "9970829603a54ac895234bed2312d1dd",
      "1824d10805ba4475be1be6c84a48a135",
      "455ad740f96a4695973f87310511c66f",
      "e034a829d09647b9b544b457c1fa35f1",
      "10e6522e23f442ff87a93fd1e9896c46",
      "6b90d5ad01594df6875ec973c238a9c8",
      "ab476d6008dd48a8b80ce8afd67cdf19",
      "eb1658f70c954bcc90e5bad86597c113",
      "7284e902b9b74c58b1a60ab37efdb9e9",
      "0d4e8a2fe947485da54052c7dca4db3a"
     ]
    },
    "id": "YqzDbFBWDJcu",
    "outputId": "ce31cd07-eb21-40e3-b020-fce8734f3af9"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for the inference pipeline.\n",
    "    \"\"\"\n",
    "    # First authenticate with Hugging Face\n",
    "    print(\"=== Hugging Face Authentication ===\")\n",
    "    hf_token = authenticate_huggingface()\n",
    "    if not hf_token:\n",
    "        print(\"Authentication failed. Please check your token and try again.\")\n",
    "        return\n",
    "\n",
    "    # Verify all required files exist\n",
    "    if not verify_files():\n",
    "        print(\"Please upload all required files to continue.\")\n",
    "        return\n",
    "\n",
    "    # Configuration\n",
    "    adapter_path = './'  # Path to your LoRA adapters\n",
    "    base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Base model name\n",
    "    data_path = './hamshahri_online_10.xlsx'  # Path to the Hamshahri dataset\n",
    "    output_path = './inference_results'  # Output directory\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Persian News Title Generation using Llama 3 with LoRA Adapters\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Initialize inference pipeline with the token\n",
    "    pipeline = PersianTitleInference(adapter_path, base_model_name, hf_token=hf_token)\n",
    "\n",
    "    # Load the model with LoRA adapters\n",
    "    if not pipeline.load_model():\n",
    "        print(\"Failed to load model. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pipeline.load_hamshahri_data(data_path)\n",
    "    if df is None:\n",
    "        print(\"Failed to load data. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # Perform inference\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Starting title generation...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Generate titles using summaries (introtext or first part of article)\n",
    "    generated_titles = pipeline.batch_inference(df, input_column='summary', batch_size=4)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\nInference completed in {end_time - start_time}\")\n",
    "\n",
    "    # Save results\n",
    "    pipeline.save_results(df, generated_titles, output_path)\n",
    "\n",
    "    # Analyze results\n",
    "    pipeline.analyze_results(df)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Pipeline Completed Successfully\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
