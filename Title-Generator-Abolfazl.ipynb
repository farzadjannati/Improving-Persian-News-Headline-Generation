{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6491,
     "status": "ok",
     "timestamp": 1721248770453,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "kujt2yTIQ2nP"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq peft datasets trl bitsandbytes accelerate evaluate bert_score rouge_score IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9495,
     "status": "ok",
     "timestamp": 1721248779944,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "8tzVztrYRaLs"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import evaluate\n",
    "from huggingface_hub import login\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3014,
     "status": "ok",
     "timestamp": 1721248782929,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "fIhnyRw4Q5Hp",
    "outputId": "261491c8-07ac-4dca-f58b-2a2e4deb1f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# login to huggingface\n",
    "my_token = ''\n",
    "login(my_token)\n",
    "\n",
    "# mount Google Drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1721248782929,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "q79G9XamRC53"
   },
   "outputs": [],
   "source": [
    "class TitleGeneratorModel:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_MESSAGE = \"you are an ai asistant to generate persian title for given article\"\n",
    "\n",
    "    TRAIN_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    {context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    {answer}<|eot_id|>\"\"\"\n",
    "\n",
    "    TEST_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    {context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "    SYSTEM_TAG_START = '<|start_header_id|>assistant<|end_header_id|>'\n",
    "\n",
    "    EOT_ID = \"<|eot_id|>\"\n",
    "\n",
    "    def __init__(self, model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, quantization_type=\"4bit\", device=None):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.dtype = dtype\n",
    "        self.quantization_type = quantization_type\n",
    "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.base_model_loaded = False\n",
    "        self.peft_model_loaded = False\n",
    "\n",
    "    def load_data(self, extract_path=\"./\", input_of_model=\"summary\", output_of_model=\"title\", only_return_data_frames = False, only_return_test_data_as_list=False):\n",
    "\n",
    "        url = 'https://huggingface.co/datasets/HooshvareLab/pn_summary/resolve/main/data/pn_summary.zip'\n",
    "        dataset_zip_file_name = extract_path+'pn_summary.zip'\n",
    "\n",
    "        if not os.path.isdir(extract_path+'pn_summary/'):\n",
    "            response = requests.get(url)\n",
    "            with open(dataset_zip_file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            with zipfile.ZipFile(dataset_zip_file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "\n",
    "        train_data = pd.read_csv(extract_path+'pn_summary/'+'train.csv', sep='\\t')\n",
    "        val_data   = pd.read_csv(extract_path+'pn_summary/'+'dev.csv', sep='\\t')\n",
    "        test_data  = pd.read_csv(extract_path+'pn_summary/'+'test.csv', sep='\\t')\n",
    "\n",
    "        if only_return_data_frames:\n",
    "            return train_data, val_data, test_data\n",
    "\n",
    "        if only_return_test_data_as_list:\n",
    "            return test_data[\"article\"].to_list(), test_data[\"summary\"].to_list(), test_data[\"title\"].to_list()\n",
    "\n",
    "        _data = []\n",
    "        for index, row in train_data.iterrows():\n",
    "            # article = row[\"article\"]\n",
    "            # summary = row[\"summary\"]\n",
    "            # title = row[\"title\"]\n",
    "            input_data = row[input_of_model]\n",
    "            output_data = row[output_of_model]\n",
    "            _data.append({\"text\":self.TRAIN_TEMPLATE.format(context=self.SYSTEM_MESSAGE, question=input_data, answer=output_data)})\n",
    "        train_dataset = Dataset.from_list(_data)\n",
    "\n",
    "        _data = []\n",
    "        for index, row in val_data.iterrows():\n",
    "            # article = row[\"article\"]\n",
    "            # summary = row[\"summary\"]\n",
    "            # title = row[\"title\"]\n",
    "            input_data = row[input_of_model]\n",
    "            output_data = row[output_of_model]\n",
    "            _data.append({\"text\":self.TRAIN_TEMPLATE.format(context=self.SYSTEM_MESSAGE, question=input_data, answer=output_data)})\n",
    "        val_dataset = Dataset.from_list(_data)\n",
    "\n",
    "        _data = []\n",
    "        for index, row in test_data.iterrows():\n",
    "            # article = row[\"article\"]\n",
    "            # summary = row[\"summary\"]\n",
    "            # title = row[\"title\"]\n",
    "            input_data = row[input_of_model]\n",
    "            output_data = row[output_of_model]\n",
    "            _data.append({\"text\":self.TRAIN_TEMPLATE.format(context=self.SYSTEM_MESSAGE, question=input_data, answer=output_data)})\n",
    "        test_dataset = Dataset.from_list(_data)\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset, train_data, val_data, test_data\n",
    "\n",
    "    def load_base_model(self):\n",
    "\n",
    "        if self.base_model_loaded:\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "\n",
    "            if self.quantization_type.lower() == \"4bit\":\n",
    "                quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=self.dtype, bnb_4bit_use_double_quant=True)\n",
    "            elif self.quantization_type.lower() == \"8bit\":\n",
    "                quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name,)\n",
    "\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "            self.model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching\n",
    "\n",
    "            self.base_model_loaded = True\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading base model: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_peftmodel(self, path):\n",
    "\n",
    "        # if self.peft_model_loaded:\n",
    "        #     return True\n",
    "\n",
    "        try:\n",
    "            self.model = PeftModel.from_pretrained(self.model, path)\n",
    "            self.model = self.model.merge_and_unload()\n",
    "            self.peft_model_loaded = True\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading peft model: {e}\")\n",
    "            return False\n",
    "\n",
    "    def train(self, epochs = 1, batch_size=4, max_seq_length=128, input_of_model=\"summary\", output_of_model=\"title\"):\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset, train_data, val_data, test_data = self.load_data(input_of_model = input_of_model, output_of_model = output_of_model)\n",
    "\n",
    "        self.load_base_model()\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "        Lora_config = LoraConfig(r=8,lora_alpha=32,lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj'])\n",
    "        os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "        training_arguments = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                # evaluation_strategy=\"steps\",\n",
    "                do_eval=False, # True\n",
    "                per_device_train_batch_size=batch_size, # 4\n",
    "                gradient_accumulation_steps=1,\n",
    "                # per_device_eval_batch_size=1, # 4\n",
    "                log_level=\"debug\",\n",
    "                optim=\"paged_adamw_32bit\",\n",
    "                save_steps=100,\n",
    "                logging_steps=100,\n",
    "                learning_rate=1e-4,\n",
    "                # eval_steps=5,\n",
    "                fp16=True,\n",
    "                max_grad_norm=0.3,\n",
    "                num_train_epochs=epochs,\n",
    "                # max_steps=100,\n",
    "                warmup_ratio=0.03,\n",
    "                lr_scheduler_type=\"constant\",\n",
    "        )\n",
    "        trainer = SFTTrainer(\n",
    "                model=self.model,\n",
    "                train_dataset=train_dataset,\n",
    "                # eval_dataset=val_dataset,\n",
    "                peft_config=Lora_config,\n",
    "                dataset_text_field=\"text\",\n",
    "                max_seq_length=max_seq_length,\n",
    "                tokenizer=self.tokenizer,\n",
    "                args=training_arguments,\n",
    "        )\n",
    "        self.train_results = trainer.train()\n",
    "        return self.train_results\n",
    "\n",
    "    def test(self, max_new_tokens=128, input_of_model=\"summary\", output_of_model=\"title\", load_top=0, return_only_one_title = True):\n",
    "\n",
    "        if not self.peft_model_loaded:\n",
    "            print(\"please load peft model before testing.\")\n",
    "            return\n",
    "\n",
    "        train_data, val_data, test_data = self.load_data(only_return_data_frames=True)\n",
    "        inputs = test_data[input_of_model].to_list()\n",
    "        outputs = test_data[output_of_model].to_list()\n",
    "\n",
    "        if load_top != 0:\n",
    "            inputs = inputs[:load_top]\n",
    "            outputs = outputs[:load_top]\n",
    "\n",
    "        generated_titles = []\n",
    "        for i in tqdm(range(len(inputs)), desc=f\"Processing {input_of_model}\"):\n",
    "            generated_title = self.__call__(inputs=inputs[i], max_new_tokens=max_new_tokens, return_only_one_title=return_only_one_title)[0]\n",
    "            generated_titles.append(generated_title)\n",
    "\n",
    "            with open(\"./generated_titles.txt\", \"a\") as outfile:\n",
    "                outfile.write(generated_title + \"\\n\")\n",
    "\n",
    "        return inputs, outputs, generated_titles\n",
    "\n",
    "    def __call__(self, inputs, max_new_tokens=128, return_only_one_title = True):\n",
    "\n",
    "        if not self.base_model_loaded:\n",
    "            self.load_base_model()\n",
    "\n",
    "        if not self.peft_model_loaded:\n",
    "            warnings.warn(\"peft model not loaded yet!\", category=Warning)\n",
    "\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        prompts = [self.TEST_TEMPLATE.format(context=self.SYSTEM_MESSAGE, question=inputs[i]) for i in range(len(inputs))]\n",
    "        tokenized_chat = self.tokenizer(prompts, return_tensors=\"pt\") # , padding=True, truncation=True\n",
    "        tokenized_chat = tokenized_chat.to(self.device)\n",
    "        generate_ids = self.model.generate(tokenized_chat[\"input_ids\"], attention_mask=tokenized_chat[\"attention_mask\"], pad_token_id=self.tokenizer.eos_token_id, max_new_tokens=max_new_tokens)\n",
    "        responses = []\n",
    "        for ids in generate_ids:\n",
    "            response = self.tokenizer.decode(ids)\n",
    "            response = response[response.index(self.SYSTEM_TAG_START)+len(self.SYSTEM_TAG_START):].strip().replace(self.EOT_ID,'').replace('/','\\n').replace('+','\\n').replace('|','\\n').replace('[n]','\\n').split('\\n')\n",
    "            response = response[0] if return_only_one_title else '|'.join(response)\n",
    "            responses.append(response)\n",
    "        return responses\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path, model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\", dtype = torch.bfloat16, quantization_type = \"4bit\", device = None):\n",
    "        try:\n",
    "\n",
    "            instance = cls(model_name=model_name, dtype=dtype, quantization_type= quantization_type, device=device)\n",
    "            if instance.load_base_model():\n",
    "                if instance.load_peftmodel(path=path):\n",
    "                    return instance\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from_pretrained: {e}\")\n",
    "            return None\n",
    "\n",
    "    @classmethod\n",
    "    def read_generated_titles_file(cls, file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1721248782930,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "9Hw9X8nQR3bq"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/My Drive/Python/abolfazl-final/summary-to-title'\n",
    "generated_titles_file_path = '/content/drive/My Drive/Python/abolfazl-final/generated_titles.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "d95751fcde6143408736f19e6da97619",
      "b6c371f175fe41db8b2113958c2026f6",
      "c7cf44b75c7e41ed9f4c58c45aa336ad",
      "babb8018771a4bf0b2dc991c09817d26",
      "ba670f3c08dd4f1694ebfee4540959ba",
      "66d824a66a074d78bb0ce7b6e1fbb57c",
      "18815aa4d74c4419af9ea9496e172180",
      "b5d0006fe12d4bf68cd14a5951aa0941",
      "c17b66982d81429d844c6d74ca9a394a",
      "e51a0fcc65234a42891a376f07470114",
      "d1552c73acc745da883477efa98336aa"
     ]
    },
    "executionInfo": {
     "elapsed": 108674,
     "status": "ok",
     "timestamp": 1721248891596,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "VdY9PF-mZ2hY",
    "outputId": "18efb3d5-2851-4846-9b29-24342d779b6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95751fcde6143408736f19e6da97619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load fine-tuned model\n",
    "model = TitleGeneratorModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1721248891597,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "Pb4T39-nSrre"
   },
   "outputs": [],
   "source": [
    "# train: : about 30 hours on summaries as input\n",
    "# model = TitleGeneratorModel()\n",
    "# model.train(epochs = 1, batch_size=4, max_seq_length=128, input_of_model=\"summary\", output_of_model=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14482,
     "status": "ok",
     "timestamp": 1721249021121,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "ldhjxNLFS36A",
    "outputId": "6d994b6a-85af-4089-9c64-ffd97f9c6075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اصرار لبنان بر حاکمیت بر اراضی و آبهای خود در مذاکرات ترسیم مرز دریایی جنوب لبنان با رژیم صهیونیستی \n"
     ]
    }
   ],
   "source": [
    "# sample use of pretrained model to generate title:\n",
    "# model = TitleGeneratorModel.from_pretrained(model_path)\n",
    "article = \"رئیس جمهور لبنان در دیدار با رئیس هیات آمریکایی میانجی‌گر مذاکرات غیرمستقیم ترسیم مرز دریایی جنوب لبنان با رژیم صهیونیستی، گفت: لبنان بر حاکمیت بر اراضی و آبهای خود اصرار دارد.\"\n",
    "print(model(article)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1721248906962,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "SuX1tdJbTRa1"
   },
   "outputs": [],
   "source": [
    "# test entite test_dataset : about 11 hours\n",
    "# model = TitleGeneratorModel.from_pretrained(model_path)\n",
    "# inputs, titles, generated_titles = model.test(max_new_tokens=64, input_of_model=\"summary\", load_top=0)\n",
    "# print(len(inputs), len(titles), len(generated_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30889,
     "status": "ok",
     "timestamp": 1721248937824,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "DTo24txdTf_D",
    "outputId": "c6fb938f-bae9-4e99-cd26-1eb5190bcd5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert-Score:\n",
      "precision: 0.797476650131595, recall: 0.8075003114024578, f1: 0.8018598676729602\n",
      "hashcode: bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.42.4)\n",
      "\n",
      "\n",
      "Rouge:\n",
      "{'rouge1': 0.006526014661183623, 'rouge2': 0.00017879492222420883, 'rougeL': 0.006526014661183621, 'rougeLsum': 0.006555813814887657}\n",
      "\n",
      "\n",
      "Bleu:\n",
      "{'bleu': 0.18660108043988477, 'precisions': [0.3905652022646045, 0.2222202945819816, 0.1432084534101825, 0.09754627636676712], 'brevity_penalty': 1.0, 'length_ratio': 1.2122385598987788, 'translation_length': 63234, 'reference_length': 52163}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate measures:\n",
    "\n",
    "# load test data and generated_titles\n",
    "articles, summaries, titles = TitleGeneratorModel().load_data(only_return_test_data_as_list=True)\n",
    "generated_titles = TitleGeneratorModel.read_generated_titles_file(generated_titles_file_path)\n",
    "\n",
    "# Bert-Score:\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "results = bertscore.compute(predictions=generated_titles, references=titles, lang=\"fa\")\n",
    "print(f\"Bert-Score:\\nprecision: {np.mean(results['precision'])}, recall: {np.mean(results['recall'])}, f1: {np.mean(results['f1'])}\\nhashcode: {results['hashcode']}\\n\\n\")\n",
    "\n",
    "# output:\n",
    "# precision: 0.797476650227508, recall: 0.8075003109335495, f1: 0.8018598673852211\n",
    "# hashcode: bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.42.4)\n",
    "\n",
    "\n",
    "# Rouge:\n",
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=generated_titles, references=titles)\n",
    "print(f'Rouge:\\n{results}\\n\\n')\n",
    "\n",
    "# output:\n",
    "# {'rouge1': 0.006466416353775554, 'rouge2': 0.00017879492222420883, 'rougeL': 0.006496215507479588, 'rougeLsum': 0.006466416353775554}\n",
    "\n",
    "# Bleu-{1,2,3,4}:\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=generated_titles, references=[[ref] for ref in titles])\n",
    "print(f'Bleu:\\n{results}\\n')\n",
    "\n",
    "# output:\n",
    "# {'bleu': 0.18660108043988477, 'precisions': [0.3905652022646045, 0.2222202945819816, 0.1432084534101825, 0.09754627636676712], 'brevity_penalty': 1.0, 'length_ratio': 1.2122385598987788, 'translation_length': 63234, 'reference_length': 52163}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20482,
     "status": "ok",
     "timestamp": 1721249042762,
     "user": {
      "displayName": "Reza Marvasti Nejad",
      "userId": "11446867734759593553"
     },
     "user_tz": -210
    },
    "id": "6ozyjK9AT8Nc",
    "outputId": "066179f9-b976-49a7-d386-d1df48767ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article:\n",
      "ایمان افتخاری در گفتگو با خبرنگار مهر در خصوص برنامه‌های حمایتی این صندوق از پژوهشگران گفت: ما ۲ برنامه حمایتی ویژه داریم که به آن هم امیدوار هستیم تا محققان کشور بتوانند پروژه‌های خود را عملی کنند. [n] رئیس صندوق حمایت از پژوهشگران و فناوران با اشاره بهکی از برنامه‌های صندوق با چین گفت: ما در سال گذشته برنامه مشترکی با اکادمی علوم چین داشتیم تا فضای همکاری مشترک محققان در دو کشور شکل بگیرد. [n] وی با بیان اینکه از چند سال پیش ارتباطات قابل توجهی با فضای پژوهشی کشور چین برقرار کردیم، گفت: بنا داشتیم فراخوانی مشترک برای اجرای ۶ طرح اولویت دار را منتشر کنیم که بیماری کرونا شیوع پیدا کرد. [n] افتخاری با بیان اینکه محورهای این فراخوان از قبل مشخص شده بود، بیان کرد: با توجه به شیوع کرونا بنا را بر این گذاشتیم که علاوه بر ۶ طرح، ۶ طرح اولویت دار دیگر در زمینه کرونا در این فراخوان قرار بگیرد. [n] وی گفت: این طرح‌ها بعد از تصویب به صورت بین المللی پیش می‌رود که پروسه پژوهشی آنها توسط صندوق مورد حمایت قرار خواهد گرفت. [n] رئیس صندوق حمایت از پژوهشگران و فناوران با اشاره به حمایت‌های صندوق از پژوهش‌ها در سطح ملی گفت: اولین فراخوان موضوعی ناظر به چالش‌های فضای عمومی کشور در زمینه کرونا را تعریف کردیم. [n] وی با بیان اینکه با توجه به شرایط کشور فراخوان در زمینه چالش کرونا مطرح شد، گفت: سعی کردیم همه جوانب مرتبط با کرونا را پوشش بدهیم؛ پروپوزال‌ها با محوریت کرونا که در بعد درمان نیستند در صندوق برای حمایت به تصویب می‌رسند و تا انتهای انجام طرح پژوهشی مورد حمایت صندوق قرار خواهند گرفت. [n] رئیس صندوق حمایت از پژوهشگران و فناوران با تاکید بر اینکه درگیری ما با این موضوع کرونا طولانی مدت است و باید به افق‌های دراز مدت توجه شود، عنوان کرد: پتانسیلی که در صندوق وجود دارد این است که از طرح‌های برجسته حمایت کنیم و برای آنها بسته‌های حمایتی در نظر بگیریم. [n] افتخاری با بیان اینکه برخی مسائل نیاز است که با دانش محققان حل شود که نیاز به واکنش‌های سریع حمایتی دارد گفت: صندوق از همان ابتدا به این موضوع وارد شده و از محققان تا قبل از رسیدن به نمونه آزمایشگاهی حمایت می‌کند.\n",
      "summary:\n",
      "رئیس صندوق حمایت از پژوهشگران و فناوران با بیان اینکه از پروژه‌های اولویت دار در سطح ملی و بین المللی حمایت می‌کنیم گفت: قرار است چند طرح با همکاری محققان چینی انجام بگیرد.\n",
      "title:\n",
      "اجرای طرح‌های اولویت دار با چین/ بسته حمایتی برای مقابله با کرونا\n",
      "generated_title:\n",
      "صندوق حمایت از پژوهشگران و فناوران چند پروژه با همکاری چینی‌ها را اجرا می‌کند\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random test\n",
    "# model = TitleGeneratorModel.from_pretrained(model_path)\n",
    "articles, summaries, titles = model.load_data(only_return_test_data_as_list=True)\n",
    "random_id = np.random.randint(len(articles))\n",
    "article, summary, title = articles[random_id], summaries[random_id], titles[random_id]\n",
    "generated_titles = model(summary, max_new_tokens=128)\n",
    "print(f'article:\\n{article}\\nsummary:\\n{summary}\\ntitle:\\n{title}\\ngenerated_title:\\n{generated_titles[0]}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMJXrVM4LxjaF3jz5MTrPfl",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18815aa4d74c4419af9ea9496e172180": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66d824a66a074d78bb0ce7b6e1fbb57c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5d0006fe12d4bf68cd14a5951aa0941": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6c371f175fe41db8b2113958c2026f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66d824a66a074d78bb0ce7b6e1fbb57c",
      "placeholder": "​",
      "style": "IPY_MODEL_18815aa4d74c4419af9ea9496e172180",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ba670f3c08dd4f1694ebfee4540959ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "babb8018771a4bf0b2dc991c09817d26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e51a0fcc65234a42891a376f07470114",
      "placeholder": "​",
      "style": "IPY_MODEL_d1552c73acc745da883477efa98336aa",
      "value": " 4/4 [01:13&lt;00:00, 15.74s/it]"
     }
    },
    "c17b66982d81429d844c6d74ca9a394a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7cf44b75c7e41ed9f4c58c45aa336ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5d0006fe12d4bf68cd14a5951aa0941",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c17b66982d81429d844c6d74ca9a394a",
      "value": 4
     }
    },
    "d1552c73acc745da883477efa98336aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d95751fcde6143408736f19e6da97619": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6c371f175fe41db8b2113958c2026f6",
       "IPY_MODEL_c7cf44b75c7e41ed9f4c58c45aa336ad",
       "IPY_MODEL_babb8018771a4bf0b2dc991c09817d26"
      ],
      "layout": "IPY_MODEL_ba670f3c08dd4f1694ebfee4540959ba"
     }
    },
    "e51a0fcc65234a42891a376f07470114": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
