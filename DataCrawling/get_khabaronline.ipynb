{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323c0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "KEYWORDS = {\"ببینید\", \"فیلم\", \"عکس\", \"ویدیو\", \"ویدئو\", \"تصاویر\", \"تصاویری\"}\n",
    "\n",
    "def _normalize_persian(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return (text.replace(\"ي\", \"ی\")\n",
    "                .replace(\"ك\", \"ک\")\n",
    "                .replace(\"ۀ\", \"ه\"))\n",
    "\n",
    "def read_existing_news_ids(csv_file: str) -> set:\n",
    "    if not os.path.isfile(csv_file):\n",
    "        return set()\n",
    "\n",
    "    existing_ids = set()\n",
    "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader, None)\n",
    "        if header:\n",
    "            news_id_index = header.index(\"news_id\") if header else 0\n",
    "\n",
    "        else:\n",
    "            print(\"Header is missing.\")\n",
    "            return set()\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) > news_id_index:\n",
    "                existing_ids.add(row[news_id_index])\n",
    "    \n",
    "    return existing_ids\n",
    "\n",
    "\n",
    "def get_news(url: str) -> str:\n",
    "    r = requests.get(url, allow_redirects=True, timeout=20)\n",
    "\n",
    "    # Check if the HTTP status code is 404\n",
    "    if r.status_code == 404:\n",
    "        print(f\"Skipped: {url} - 404 Not Found\")\n",
    "        return None  # Skip if 404 error\n",
    "\n",
    "    r.raise_for_status()  # Raise an exception for other HTTP errors\n",
    "    return r.text\n",
    "\n",
    "def parse_news_html(html: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # news_id\n",
    "    news_id_tag = soup.find(\"li\", class_=\"id\")\n",
    "    news_id = news_id_tag.find(\"span\").get_text(strip=True) if news_id_tag else \"\"\n",
    "\n",
    "    # category\n",
    "    breadcrumb = soup.find(\"ol\", class_=\"breadcrumb\")\n",
    "    category = _normalize_persian(breadcrumb.find_all(\"a\", rel=\"index\")[1].get_text(strip=True)) if breadcrumb else \"\"\n",
    "\n",
    "    # date\n",
    "    date_span = soup.select_one(\"div.item-date > span\")\n",
    "    date = _normalize_persian(date_span.get_text(strip=True)) if date_span else \"\"\n",
    "\n",
    "    # url + title\n",
    "    h1 = soup.find(\"h1\", class_=\"title\")\n",
    "    a_title = h1.find(\"a\") if h1 else None\n",
    "    url = (\"https://www.khabaronline.ir\" + a_title[\"href\"]) if (a_title and a_title.has_attr(\"href\")) else \"\"\n",
    "    title = _normalize_persian(a_title.get_text(strip=True)) if a_title else \"\"\n",
    "\n",
    "    # introtext\n",
    "    intro = soup.find(\"p\", class_=\"introtext\")\n",
    "    introtext = _normalize_persian(intro.get_text(\" \", strip=True)) if intro else \"\"\n",
    "\n",
    "    # news text\n",
    "    text_lines = []\n",
    "    content_root = soup.select_one(\".item-body\") or soup\n",
    "    stop = False\n",
    "    for node in content_root.descendants:\n",
    "        if stop:\n",
    "            break\n",
    "        if getattr(node, \"name\", None) is None:\n",
    "            continue\n",
    "        if node.name == \"div\" and \"item-code\" in (node.get(\"class\") or []):\n",
    "            stop = True\n",
    "            break\n",
    "        if node.name in {\"p\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"}:\n",
    "            if node.find_parent([\"ul\", \"ol\"]) is not None:\n",
    "                continue\n",
    "            txt = node.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                text_lines.append(_normalize_persian(txt))\n",
    "    \n",
    "    # delete last <p> without any info\n",
    "    if text_lines and re.match(r\"^\\d[\\d\\s]*$\", text_lines[-1]):\n",
    "        text_lines = text_lines[:-1]\n",
    "\n",
    "    # text = \"\\n\".join(text_lines)\n",
    "    text = \" [n] \".join(text_lines)\n",
    "\n",
    "    # tags\n",
    "    tags_section = soup.select(\"section.box.tags a[rel=tag]\")\n",
    "    tags = \",\".join([_normalize_persian(a.get_text(strip=True)) for a in tags_section]) if tags_section else \"\"\n",
    "\n",
    "    return {\n",
    "        \"news_id\": news_id,\n",
    "        \"category\": category,\n",
    "        \"date\": date,\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"introtext\": introtext,\n",
    "        \"text\": text,\n",
    "        \"tags\": tags,\n",
    "    }\n",
    "\n",
    "def _should_skip(row_dict: dict) -> bool:\n",
    "    \"\"\"Condition for completely skipping the news from CSV\"\"\"\n",
    "\n",
    "    title = row_dict.get(\"title\", \"\") or \"\"\n",
    "    text = row_dict.get(\"text\", \"\") or \"\"\n",
    "    introtext = row_dict.get(\"introtext\", \"\") or \"\"\n",
    "\n",
    "    # Condition 1: Forbidden words in the title\n",
    "    if any(kw in title for kw in KEYWORDS):\n",
    "        return True\n",
    "\n",
    "    # Condition 2: Empty introtext\n",
    "    if not introtext.strip():\n",
    "        return True\n",
    "\n",
    "    # Condition 3: Text word count less than 10\n",
    "    word_count = len(re.findall(r\"\\S+\", text))\n",
    "    if word_count < 10:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def write_news_csv(row_dict: dict, csv_file: str = \"news.csv\"):\n",
    "    headers = [\"news_id\", \"category\", \"date\", \"url\", \"title\", \"introtext\", \"text\", \"tags\"]\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    need_header = (not file_exists) or os.path.getsize(csv_file) == 0\n",
    "\n",
    "    with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if need_header:\n",
    "            w.writerow(headers)\n",
    "        w.writerow([row_dict.get(h, \"\") for h in headers])\n",
    "\n",
    "def parse_news_from_url(url: str, csv_file: str = \"news.csv\", news_id: str = \"\"):\n",
    "    # Checking news_id's in CSV\n",
    "    existing_ids = read_existing_news_ids(csv_file)\n",
    "\n",
    "    if str(news_id) in existing_ids:\n",
    "        print(f\"Existed: {news_id} - Already exists in CSV.\")\n",
    "        return None\n",
    "    \n",
    "    html = get_news(url)\n",
    "\n",
    "    if html is None:\n",
    "        return None\n",
    "    \n",
    "    row = parse_news_html(html)\n",
    "\n",
    "    if _should_skip(row):\n",
    "        print(f\"Skipped: {row['news_id']} ({row['title']})\")\n",
    "        return None\n",
    "\n",
    "    write_news_csv(row, csv_file)\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7054428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: 2105756 (ببینید |  تصاویر تازه‌تر از حملات تروریستی امروز در ایرانشهر)\n",
      "Skipped: 2105772 (ببینید | تصاویر مردم‌آزاری با هلی‌کوپتر شخصی در ساحل شمال ایران!)\n",
      "Skipped: 2105804 (ببینید | مغانلو پنالتی گرفت؛ سامان گل زد!)\n",
      "Skipped: https://www.khabaronline.ir/news/2105876 - 404 Not Found\n",
      "Skipped: https://www.khabaronline.ir/news/2105884 - 404 Not Found\n",
      "Skipped: https://www.khabaronline.ir/news/2105892 - 404 Not Found\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_id = 2105700\n",
    "    end_id   = 2105900\n",
    "    csv_file = \"news_khabaronline_20.csv\"\n",
    "\n",
    "    for news_id in range(start_id, end_id + 1, 8):\n",
    "        url = f\"https://www.khabaronline.ir/news/{news_id}\"\n",
    "        parse_news_from_url(url, csv_file, news_id)\n",
    "        # try:\n",
    "        #     data = parse_news_from_url(url, csv_file, news_id)\n",
    "        #     print(f\"Done {news_id}\")\n",
    "        #     for k, v in data.items():\n",
    "        #         print(f\"{k}: {v}\")\n",
    "        #     print(\"-\" * 50)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error in {news_id}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
